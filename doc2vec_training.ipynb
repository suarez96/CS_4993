{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if NOT working in colab\n",
    "data_dir = './data'\n",
    "\n",
    "# if working in colab\n",
    "# data_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill_type = pd.read_csv(os.path.join(data_dir, 'NOC_skilltype.csv'))\n",
    "df_major_group = pd.read_csv(os.path.join(data_dir, './NOC_majorgroup.csv'))\n",
    "df_minor_group = pd.read_csv(os.path.join(data_dir, './NOC_minorgroup.csv'))\n",
    "df = pd.read_csv(os.path.join(data_dir, './noc_data_get_byws_dealing_slash.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad missing digits from noc codes\n",
    "df['Noc_code'] = df['Noc_code'].apply(lambda x: '{0:0>4}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unusual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "ocurrences = 0\n",
    "\n",
    "def find_character(string, char):\n",
    "    global ocurrences\n",
    "    for occ in string.split(';'):\n",
    "        if char in occ:\n",
    "            ocurrences += 1\n",
    "        \n",
    "df['job_title'].apply(find_character, args=('.'))\n",
    "\n",
    "print(ocurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is abbreviation, value is expanded occupation\n",
    "abbreviations_map = {}\n",
    "\n",
    "def handle_single_quotes(text):\n",
    "    \"\"\"\n",
    "    handle plurals, which are the main use of the single quote. Afterwards, drop all other single quotes\n",
    "    \"\"\"\n",
    "    text = text.replace(\"s'\", '').replace(\"'s\", '')\n",
    "    return text.replace(\"'\", '')\n",
    "\n",
    "def handle_parentheses(text, strip_abbrev):\n",
    "    \"\"\"\n",
    "    Parentheses seem to fall into two general cases in the VAST majority of instances:\n",
    "    1. Indicates an abbreviation\n",
    "    2. Indicates an exception, by using keywords such as \"except\" or \"non\"\n",
    "    \"\"\"\n",
    "    parentheses_idx = 0\n",
    "    split = text.split(\"(\")\n",
    "    for i, substr in enumerate(split):\n",
    "        if ')' in substr:\n",
    "            parentheses_idx = i\n",
    "            break\n",
    "    \n",
    "    # fragment before the fragment with the paren.\n",
    "    str1 = split[parentheses_idx-1].strip()\n",
    "    assert not ')' in str1\n",
    "    \n",
    "    # fragment w parenthesis\n",
    "    str2 = split[parentheses_idx].split(\")\")[0].strip()\n",
    "    \n",
    "    if 'except' in str2 or 'non' in str2:\n",
    "        text = text.replace(str2, '')\n",
    "        # TODO, do something with exceptions\n",
    "        \n",
    "    else:\n",
    "        # take the shorter string as the abbreviation\n",
    "        ab, ex = (str1, str2) if len(str1) < len(str2) else (str2, str1)\n",
    "    \n",
    "        # save abbreviation\n",
    "        abbreviations_map[ab] = ex\n",
    "        \n",
    "        # remove the found abbreviation from job title\n",
    "        if strip_abbrev:\n",
    "            text = text.replace(ab, '')\n",
    "\n",
    "    # remove parentheses, leading and trailing whitespace \n",
    "    text = text.replace('(','').replace(')','').strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, strip_abbrev=False):\n",
    "    \n",
    "    # handle slashes\n",
    "    text = text.replace(\"/\", ' ')\n",
    "    \n",
    "    # remove redundant semi-colons\n",
    "    text = text.strip(';')\n",
    "    \n",
    "    # hyphens are semantic noise, remove\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # handle '\n",
    "    if \"'\" in text:\n",
    "        text = handle_single_quotes(text)\n",
    "    \n",
    "    # handle ,\n",
    "    text = text.replace(\",\", '')\n",
    "    \n",
    "    # handle .\n",
    "    text = text.replace(\".\", '')\n",
    "    \n",
    "    # handle parentheses, only one check necessary since we already verified they are all paired with corresponding ')'\n",
    "    if \"(\" in text:\n",
    "        text = handle_parentheses(text, strip_abbrev=strip_abbrev)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # normalize case\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job_samples = {}\n",
    "\n",
    "def extract_job_samples(row):\n",
    "    NOC_code = int(row['Noc_code'])\n",
    "    \n",
    "    # split jobs contained in row by ';' and .replace('-', '; ') is for '-', .replace('-', '; ')\n",
    "    # REVISE WHETHER TO KEEP - separation. logic is that lieutenant-governor can be described as lieutenant governer, no hyphen\n",
    "    # make unique set\n",
    "    # strip extra characters \n",
    "    # and take nonempty elements\n",
    "    jobs = [\n",
    "        j for j in  row['job_title'].split(';')\n",
    "        if (j != '' and j != ' ')\n",
    "    ]\n",
    "    \n",
    "    # change gendered entries such as 'chairman/woman' into separate samples, 'chairman', 'chairwoman'\n",
    "    for idx, job in enumerate(jobs):\n",
    "        if 'man/woman' in job:\n",
    "            # change original entry to 'job(man)', then append job(woman) to end of list\n",
    "            jobs[idx] = job.replace('man/woman', 'man')\n",
    "            jobs.append(job.replace('man/woman', 'woman'))\n",
    "        if 'men/women' in job:\n",
    "            jobs[idx] = job.replace('men/women', 'men')\n",
    "            jobs.append(job.replace('men/women', 'women'))\n",
    "        if 'boy/girl' in job:\n",
    "            jobs[idx] = job.replace('boy/girl', 'boy')\n",
    "            jobs.append(job.replace('boy/girl', 'girl'))\n",
    "        if 'master/mistress' in job:\n",
    "            jobs[idx] = job.replace('master/mistress', 'master')\n",
    "            jobs.append(job.replace('master/mistress', 'mistress'))\n",
    "        if 'host/hostess' in job:\n",
    "            jobs[idx] = job.replace('host/hostess', 'host')\n",
    "            jobs.append(job.replace('host/hostess', 'hostess'))\n",
    "        if 'waiter/waitress' in job:\n",
    "            jobs[idx] = job.replace('waiter/waitress', 'waiter')\n",
    "            jobs.append(job.replace('waiter/waitress', 'waitress'))\n",
    "        \n",
    "    # clean text data\n",
    "    preprocessed_jobs = [preprocess_text(job) for job in jobs]\n",
    "            \n",
    "    # remove duplicate entries\n",
    "    preprocessed_jobs = set(preprocessed_jobs)\n",
    "    \n",
    "    # parse counts of each job\n",
    "    row['n_sample_jobs'] = len(preprocessed_jobs)\n",
    "    \n",
    "    # iterate through job and add to dictionary\n",
    "    for j in preprocessed_jobs:\n",
    "        \n",
    "        if j not in all_job_samples:\n",
    "            all_job_samples[j] = NOC_code\n",
    "\n",
    "        # safe check, if job appears more than once, clause will print the both NOC Codes\n",
    "        else:\n",
    "            if all_job_samples[j] != NOC_code:\n",
    "                print(j, 'repeated', all_job_samples[j], NOC_code)\n",
    "    \n",
    "    return row\n",
    "\n",
    "def parse_1(row):\n",
    "    # get info from first digit of 4 digit code\n",
    "    row['1_digit_target'] = int(str(row['Noc_code'])[0])\n",
    "    row['1_digit_group'] = df_skill_type[df_skill_type['skilltype_code'] == row['1_digit_target']]['skilltype_title']\n",
    "        \n",
    "    return row\n",
    "\n",
    "def parse_2(row):\n",
    "    # get info from first 2 digits of 4 digit code\n",
    "    \n",
    "    # check if NOC code is long enough for parsing\n",
    "    if len(str(row['Noc_code'])) > 1:\n",
    "        row['2_digit_target'] = int(str(row['Noc_code'])[:2])\n",
    "        row['2_digit_group'] = df_major_group[df_major_group['majorgroup_code'] == '\\'' + str(row['2_digit_target'])]['majorgroup_title']\n",
    "        \n",
    "    else:\n",
    "        row['2_digit_target'] = 'NA'\n",
    "        row['2_digit_group'] = 'NA'\n",
    "    \n",
    "    return row\n",
    "\n",
    "def parse_3(row):\n",
    "    # get info from first 3 digits of 4 digit code\n",
    "    \n",
    "    # check if NOC code is long enough for parsing\n",
    "    if len(str(row['Noc_code'])) > 2:\n",
    "        row['3_digit_target'] = int(str(row['Noc_code'])[:3])\n",
    "        row['3_digit_group'] = df_minor_group[df_minor_group['minorgroup_code'] == '\\'' + str(row['3_digit_target'])]['minorgroup_title']\n",
    "        \n",
    "    else:\n",
    "        row['3_digit_target'] = 'NA'\n",
    "        row['3_digit_group'] = 'NA'\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29518"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do once, if 'noc_code' column already dropped, except to skip action\n",
    "try:\n",
    "    df = df.apply(parse_1, axis = 1)\n",
    "    df = df.apply(parse_2, axis = 1)\n",
    "    df = df.apply(parse_3, axis = 1)\n",
    "    df = df.apply(extract_job_samples, axis = 1)\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "len(all_job_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = {}\n",
    "desc_counts = []\n",
    "def unpack_descriptions(row):\n",
    "    # unpack all descriptions from a row and\n",
    "    duty = row['main_duties']\n",
    "    desc_counts.append(0)\n",
    "    \n",
    "    # split duty field into separate duties and remove initial generic blurb\n",
    "    for description in duty.strip('-').split(';'):\n",
    "        if 'duties' not in description:\n",
    "            all_descriptions[description] = row['Noc_code']\n",
    "            desc_counts[-1] += 1\n",
    "            \n",
    "    return row\n",
    "\n",
    "df = df.apply(unpack_descriptions, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dictionary from selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dict(all_job_samples)\n",
    "# train_df.update(all_descriptions)\n",
    "train_df = pd.DataFrame(train_df.items(), columns = ['input', 'code'])\n",
    "train_df['input'] = train_df['input'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab sample to see if preprocessing worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(string):\n",
    "    try:\n",
    "        assert '.' not in string \\\n",
    "            and ',' not in string \\\n",
    "                and ')' not in string \\\n",
    "                    and '(' not in string \\\n",
    "                        and '-' not in string \\\n",
    "                            and ';' not in string \\\n",
    "                                and '/' not in string \\\n",
    "                                    and '\\'' not in string\n",
    "    except AssertionError:\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>map clerk</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22872</th>\n",
       "      <td>multiple spindle drill press set up operator m...</td>\n",
       "      <td>9417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8337</th>\n",
       "      <td>professor of computer science university</td>\n",
       "      <td>4011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>invoice control clerk</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6283</th>\n",
       "      <td>mine draftsperson</td>\n",
       "      <td>2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23475</th>\n",
       "      <td>tire recapper</td>\n",
       "      <td>9423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>buffing and lacquering forewoman furniture and...</td>\n",
       "      <td>9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>registered dietitian nutritionist rdn</td>\n",
       "      <td>3132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>industrial and manufacturing production manager</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>concrete forms carpenter</td>\n",
       "      <td>7271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5611</th>\n",
       "      <td>marine biology technician</td>\n",
       "      <td>2221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19349</th>\n",
       "      <td>field crop and vegetable workers foreman</td>\n",
       "      <td>8252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22639</th>\n",
       "      <td>shear operator metal fabrication</td>\n",
       "      <td>9416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21793</th>\n",
       "      <td>copper briquetting machine operator</td>\n",
       "      <td>9411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23357</th>\n",
       "      <td>plastic windshield processor</td>\n",
       "      <td>9422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>laundry production supervisor</td>\n",
       "      <td>6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10525</th>\n",
       "      <td>drummer</td>\n",
       "      <td>5133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18779</th>\n",
       "      <td>linewoman helper</td>\n",
       "      <td>7612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27235</th>\n",
       "      <td>hardware installer furniture manufacturing</td>\n",
       "      <td>9532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28484</th>\n",
       "      <td>firearms cleaner metal products manufacturing</td>\n",
       "      <td>9612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  code\n",
       "3623                                           map clerk  1411\n",
       "22872  multiple spindle drill press set up operator m...  9417\n",
       "8337            professor of computer science university  4011\n",
       "4331                               invoice control clerk  1524\n",
       "6283                                   mine draftsperson  2253\n",
       "23475                                      tire recapper  9423\n",
       "21081  buffing and lacquering forewoman furniture and...  9224\n",
       "7325               registered dietitian nutritionist rdn  3132\n",
       "2302     industrial and manufacturing production manager   911\n",
       "15344                           concrete forms carpenter  7271\n",
       "5611                           marine biology technician  2221\n",
       "19349           field crop and vegetable workers foreman  8252\n",
       "22639                   shear operator metal fabrication  9416\n",
       "21793                copper briquetting machine operator  9411\n",
       "23357                       plastic windshield processor  9422\n",
       "12126                      laundry production supervisor  6316\n",
       "10525                                            drummer  5133\n",
       "18779                                   linewoman helper  7612\n",
       "27235         hardware installer furniture manufacturing  9532\n",
       "28484      firearms cleaner metal products manufacturing  9612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['input'].apply(check)\n",
    "display(train_df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_digits(string, n=4):\n",
    "    \n",
    "    # if default number of digits desired, don't do anything\n",
    "    if n == 4:\n",
    "        return string\n",
    "    \n",
    "    # else pad left with zeros until 4 digits reached\n",
    "    padded_str = '{0:0>4}'.format(string)\n",
    "    return padded_str[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X = train_df['input']\n",
    "\n",
    "y1 = train_df['code'].apply(first_n_digits, args = (1,)).astype('int')\n",
    "y2 = train_df['code'].apply(first_n_digits, args = (2,)).astype('int')\n",
    "y3 = train_df['code'].apply(first_n_digits, args = (3,)).astype('int')\n",
    "y4 = train_df['code'].apply(first_n_digits, args = (4,)).astype('int')\n",
    "\n",
    "# select how many digits to train on\n",
    "y = y4\n",
    "\n",
    "X_train, y_train = X, y\n",
    "\n",
    "corpus = list(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For window size calculation, get mean length of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(x.split()) for x in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Doc2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(item.lower()), tags=[str(i)]) for i, item in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRIAL_NAME = 'trial_10'\n",
    "curr_model_name = \"{}.model\".format(TRIAL_NAME)\n",
    "\n",
    "epochs = 4096 # training cycles\n",
    "vec_size = 32 # specific to doc2vec, size of the output vector\n",
    "alpha = 0.001 # learning rate\n",
    "window = 3\n",
    "min_count = 2\n",
    "min_alpha = 0.00025\n",
    "\n",
    "try:\n",
    "    assert not os.path.exists(curr_model_name), \"Model {} already exists! Update model output name\".format(curr_model_name)\n",
    "\n",
    "\n",
    "    model = Doc2Vec(vector_size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    window=window,\n",
    "                    min_alpha=min_alpha,\n",
    "                    min_count= min_count,\n",
    "                    dm=1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.epochs)\n",
    "        # LR scheduling\n",
    "        model.alpha -= 0.00002\n",
    "\n",
    "    model.save(curr_model_name)\n",
    "    print(\"Model {} Saved\".format(curr_model_name))\n",
    "\n",
    "except AssertionError:\n",
    "    print(\"Existing Model {} Found\".format(curr_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "model= Doc2Vec.load(curr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_doc2vec_encoding(occ, steps=128, alpha=0.03):\n",
    "    test_data = word_tokenize(occ)\n",
    "    test_vector = model.infer_vector(test_data, steps=steps, alpha=alpha)\n",
    "    return test_vector\n",
    "\n",
    "def get_occ_and_code_from_tokens(training_doc):\n",
    "    \"\"\"\n",
    "    Return the train input in readable form as well as its corresponding NOC code\n",
    "    \"\"\"\n",
    "    tokens = tagged_data[int(training_doc[0])][0]\n",
    "    \n",
    "    detokenized_job = detokenizer.detokenize(tokens)\n",
    "        \n",
    "    try:\n",
    "        code = int(train_df[train_df['input'] == detokenized_job]['code'])\n",
    "    except TypeError:\n",
    "        try: # TEMPORARY WHILE WE FIX PARENTHESES PROBLEM\n",
    "            code = int(\n",
    "                train_df.loc[train_df['input'].str.contains(detokenized_job.split('(')[0]), 'code'].values[0]\n",
    "            )\n",
    "        except ValueError:\n",
    "            code = 0\n",
    "        except IndexError:\n",
    "            code = 0\n",
    "    \n",
    "    return detokenized_job, code\n",
    "\n",
    "def infer(str_input, verbose=False):\n",
    "    \n",
    "    preprocessed_str = preprocess_text(str_input)\n",
    "    \n",
    "    job_vector = get_doc2vec_encoding(preprocessed_str)\n",
    "    \n",
    "    # to find most similar doc using tags\n",
    "    similar_doc = model.docvecs.most_similar([job_vector])\n",
    "    \n",
    "    codes = []\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------Test on {}---------'.format(preprocessed_str))\n",
    "    \n",
    "    for doc in similar_doc:\n",
    "        \n",
    "        job, code = get_occ_and_code_from_tokens(doc)\n",
    "                \n",
    "        codes.append(code)\n",
    "        \n",
    "        if verbose:\n",
    "            print('{} - {}'.format(job, code))\n",
    "    \n",
    "    return Counter(codes)\n",
    "\n",
    "def process_counter(counter):\n",
    "    \n",
    "    if len(counter) >= 3:\n",
    "        v1, v2, v3 = (int(w) for w, c in counter.most_common(3))\n",
    "        \n",
    "    elif len(counter) == 2:\n",
    "        v1, v2  = (int(w) for w, c in counter.most_common(2))\n",
    "        v3 = 0\n",
    "        \n",
    "    elif len(counter) == 1:\n",
    "        v1 = counter.most_common(1)[0][0]\n",
    "        v2, v3 = 0, 0\n",
    "            \n",
    "    return pd.Series([v1, v2, v3])\n",
    "\n",
    "def infer_and_vote(occ, verbose=False):\n",
    "    counter = infer(occ, verbose=verbose)\n",
    "    return process_counter(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_occupations = ['doctor', 'athlete', 'member of parliament',\n",
    "                    'teacher', 'researcher', 'registered nurse', \n",
    "                    'CUSTOMER SERVICE', 'MANAGER OF CLEANING BUSINESS',\n",
    "                   'CAREGIVER', 'Farm Boss']\n",
    "\n",
    "for occ in test_occupations: \n",
    "    print(infer_and_vote(occ, verbose=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply embeddings to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d2v_embeddings = train_df['input'].apply(get_doc2vec_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_embeddings(data):\n",
    "    return np.array([list(x) for x in np.array(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['doc2vec_embeddings'] = train_d2v_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CODE_LENGTH = 4\n",
    "\n",
    "classifier_input = vectorize_embeddings(train_d2v_embeddings)\n",
    "classifer_output = np.array(train_df['code'].apply(first_n_digits, args = (TARGET_CODE_LENGTH,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build preliminary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(np.divide(classifier_input, 9).flatten()))\n",
    "print(max(classifier_input.flatten()))\n",
    "print(min(classifier_input.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(class_weight='balanced', kernel='linear')\n",
    "\n",
    "start = time.time()\n",
    "SVM.fit(classifier_input, classifer_output)\n",
    "print('SVM training duration: {} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=256, max_depth=128, n_jobs=-1, warm_start=True)\n",
    "\n",
    "start = time.time()\n",
    "RF.fit(classifier_input, classifer_output)\n",
    "print('RF training duration: {} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 1, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "KNN.fit(classifier_input, classifer_output)\n",
    "print('KNN training duration: {} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATP_data = pd.DataFrame(pd.read_excel('./Data/V5_Run Input(1).xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP = ATP_data\n",
    "trimmed_ATP['Current Job Title'] = trimmed_ATP['Current Job Title'].apply(preprocess_text)\n",
    "trimmed_ATP['Current Industry'] = trimmed_ATP['Current Industry'].apply(preprocess_text)\n",
    "trimmed_ATP['NOC code'] = trimmed_ATP['NOC code '].apply(lambda x: int(x.strip('\\''))).apply(first_n_digits, args=(TARGET_CODE_LENGTH,))\n",
    "trimmed_ATP.drop(columns = ['NOC code '], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP['vote1'], trimmed_ATP['vote2'], trimmed_ATP['vote3'] = None, None, None\n",
    "trimmed_ATP[['vote1', 'vote2' ,'vote3']] = trimmed_ATP['Current Job Title'].apply(infer_and_vote)\n",
    "TPs = trimmed_ATP.apply(lambda row: int(row['NOC code']) in [row['vote1'], row['vote2'], row['vote3']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d2v_embeddings = trimmed_ATP['Current Job Title'].apply(get_doc2vec_encoding)\n",
    "trimmed_ATP['doc2vec_embeddings'] = test_d2v_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized_embeddings = vectorize_embeddings(test_d2v_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP['rf_pred'] = RF.predict(vectorized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP['knn_pred'] = KNN.predict(vectorized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP['svm_pred'] = SVM.predict(vectorized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ATP[['knn_pred', 'svm_pred', 'rf_pred', 'NOC code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "for classifier in ['knn','svm', 'rf']:\n",
    "    print('{} acc:{}, f1-macro:{}'.format(classifier.upper(), \n",
    "                                    accuracy_score(\n",
    "                                        trimmed_ATP['{}_pred'.format(classifier)], \n",
    "                                        trimmed_ATP['NOC code']\n",
    "                                    ),\n",
    "                                    f1_score(\n",
    "                                        trimmed_ATP['{}_pred'.format(classifier)],\n",
    "                                        trimmed_ATP['NOC code'], average = 'macro')\n",
    "                                   )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm accuracy still tanks, potentially overfitting. the problem is too many output classes. \n",
    "# to mitigate, build hierarchical model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
