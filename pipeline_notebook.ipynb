{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from scripts.Embedder import Embedder, tfidfEmbedder\n",
    "from scripts.OccupationPreprocessor import OccupationPreprocessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_dig_tfidf_clfs.pkl', 'rb') as f:\n",
    "    clf1 = pickle.load(f)\n",
    "    \n",
    "with open('second_third_fourth_dig_tfidf_clfs.pkl', 'rb') as f2:\n",
    "    clf2 = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FILE == 0:\n",
    "    tfidf_test_set = OccupationPreprocessor.prepare_df(\n",
    "        './Data/ATP_gold_standard.xlsx',\n",
    "        input_column='CURRENT_JOB_TITLE',\n",
    "        code_column='NOC code by PATH',\n",
    "        n_digits=4\n",
    "    )\n",
    "    \n",
    "    NOCv5 = OccupationPreprocessor.prepare_df(\n",
    "        './Data/ATP_gold_standard.xlsx',\n",
    "        input_column='NOC code by PATH',\n",
    "        code_column='V5_NOC',\n",
    "        n_digits=4\n",
    "    )\n",
    "    \n",
    "    NOCv6 = OccupationPreprocessor.prepare_df(\n",
    "        './Data/ATP_gold_standard.xlsx',\n",
    "        input_column='NOC code by PATH',\n",
    "        code_column='V6_NOC',\n",
    "        n_digits=4\n",
    "    )\n",
    "    \n",
    "elif TEST_FILE == 1:\n",
    "    tfidf_test_set = pd.DataFrame(\n",
    "        pd.read_csv('./Data/tfidf_test_set.csv')\n",
    "    )\n",
    "    \n",
    "# A-B test set with train data\n",
    "elif TEST_FILE == 2:\n",
    "    tfidf_test_set = pd.DataFrame(\n",
    "        pd.read_csv('overlap_test_set_v4_acanoc_with_train_data.csv')\n",
    "    )\n",
    "    \n",
    "# A-B test set no train data\n",
    "elif TEST_FILE == 3:\n",
    "    tfidf_test_set = pd.DataFrame(\n",
    "        pd.read_csv('overlap_test_set_v4_acanoc_no_train_data.csv')\n",
    "    )\n",
    "    \n",
    "tfidf_train_set = pd.DataFrame(\n",
    "    pd.read_csv('./Data/tfidf_train_set.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = tfidfEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500 if TEST_FILE == 0 else 500\n",
    "sample_pipeline_df = tfidf_test_set.sample(sample_size, random_state=123)#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for alternative classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def clean_noc_double_codes(row, column):\n",
    "    if ',' in row[column]:\n",
    "        row[column] = str(row[column].strip('\\'').split(',')[1])\n",
    "    return row\n",
    "\n",
    "if TEST_FILE == 0:\n",
    "    NOCv5 = NOCv5.apply(clean_noc_double_codes, axis =1 , args = ('input',))\n",
    "    NOCv6 = NOCv6.apply(clean_noc_double_codes, axis =1 , args = ('input',))\n",
    "    print('NOCv5 Ensemble metrics total. Accuracy:{}, f1:{}\\n'.format(\n",
    "        accuracy_score(NOCv5['input'].astype(int), NOCv5['code'].astype(int)), \n",
    "        f1_score(NOCv5['input'].astype(int), NOCv5['code'].astype(int), average = 'macro')\n",
    "    ))\n",
    "    print('NOCv6 Ensemble metrics total. Accuracy:{}, f1:{}\\n'.format(\n",
    "        accuracy_score(NOCv6['input'].astype(int), NOCv6['code'].astype(int)), \n",
    "        f1_score(NOCv6['input'].astype(int), NOCv6['code'].astype(int), average = 'macro')\n",
    "    ))\n",
    "elif TEST_FILE == 2 or TEST_FILE == 3:\n",
    "    print(\n",
    "        \"Accuracy to Beat:\",\n",
    "        accuracy_score(\n",
    "            sample_pipeline_df['code'], \n",
    "            sample_pipeline_df['v4_pred']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check exact matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_vectors = embedder.embed(sample_pipeline_df['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df = pd.DataFrame({\n",
    "    'svm_pred':clf1['SVM'].predict(tfidf_test_vectors),\n",
    "    'rf_pred':clf1['RF'].predict(tfidf_test_vectors),\n",
    "    'knn_pred':clf1['KNN'].predict(tfidf_test_vectors),\n",
    "    'code':sample_pipeline_df['code'].astype(str)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df['exact_match'] = sample_pipeline_df['input'].apply(embedder.check_exact_match, args=(embedder.train_database,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df['p_all'] = tfidf_test_df.apply(tfidfEmbedder.ensemble_predict, axis = 1, args = (\n",
    "    ['rf_pred','svm_pred','knn_pred'], 'svm_pred',\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df['vectors'] = tfidf_test_vectors.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in pipeline.py\n",
    "def pipeline(row):\n",
    "    np_array = np.array(row['vectors']).reshape(1, -1)\n",
    "    p_1 = row['p_all']\n",
    "    row['svm_pred_234'] = clf2[p_1]['SVM'].predict(np_array)[0]\n",
    "    row['rf_pred_234'] = clf2[p_1]['RF'].predict(np_array)[0]\n",
    "    row['knn_pred_234'] = clf2[p_1]['KNN'].predict(np_array)[0]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "tfidf_test_df = tfidf_test_df.apply(pipeline, axis = 1)\n",
    "elapsed = time.time()-start_time\n",
    "print('time to predict on {0} samples: {1:.2f} seconds ({2:.3f} s/sample)'.format(sample_pipeline_df.shape[0],\n",
    "                                                         elapsed, \n",
    "                                                        elapsed/sample_pipeline_df.shape[0])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df['p_all_234'] = tfidf_test_df.apply(tfidfEmbedder.ensemble_predict, axis = 1, args = (\n",
    "    ['svm_pred_234','rf_pred_234','knn_pred_234'], 'knn_pred_234',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance w/o exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "for classifier in ['knn','svm', 'rf']:\n",
    "    print('{} acc:{}, f1-macro:{}'.format(classifier.upper(), \n",
    "                                    accuracy_score(\n",
    "                                        tfidf_test_df['{}_pred_234'.format(classifier)].astype(int), \n",
    "                                        tfidf_test_df['code'].astype(int)\n",
    "                                    ),\n",
    "                                    f1_score(\n",
    "                                        tfidf_test_df['{}_pred_234'.format(classifier)].astype(int),\n",
    "                                        tfidf_test_df['code'].astype(int), average = 'macro')\n",
    "                                   )\n",
    "     )\n",
    "print('Ensemble metrics total. Accuracy:{}, f1:{}\\n'.format(\n",
    "    accuracy_score(tfidf_test_df['p_all_234'].astype(int), tfidf_test_df['code'].astype(int)), \n",
    "    f1_score(tfidf_test_df['p_all_234'].astype(int), tfidf_test_df['code'].astype(int), average = 'macro')\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With exact match added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df['Hybrid_pred_exact_match'] = tfidf_test_df.apply(\n",
    "    lambda row : row['p_all_234'] if row['exact_match'] == -1 else row['exact_match'], axis = 1\n",
    ")\n",
    "print('Ensemble metrics after checking for exact match total. Accuracy:{}, f1:{}\\n'.format(\n",
    "    accuracy_score(tfidf_test_df['Hybrid_pred_exact_match'].astype(int), tfidf_test_df['code'].astype(int)), \n",
    "    f1_score(tfidf_test_df['Hybrid_pred_exact_match'].astype(int), tfidf_test_df['code'].astype(int), average = 'macro')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_df.drop(columns =['vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
